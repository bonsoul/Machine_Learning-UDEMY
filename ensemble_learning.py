# -*- coding: utf-8 -*-
"""Ensemble Learning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MsPhrSQUnuHVwNXJ8CVlxOnYosJG7G8w
"""

import pandas as pd

# Updated URL pointing to the raw CSV data
url = "https://raw.githubusercontent.com/nikhilsthorat03/Telco-Customer-Churn/main/telco.csv"

df = pd.read_csv(url)

df.head()

# display dataset info and preview

print("Dataset Info: \n")
print(df.info())
print("\n Class Distribution: \n")
print(df.Churn.value_counts())
print("\n Sample Data: \n", df.head())

#handling missing values

df.isnull().sum()

#encode categorical values

from sklearn.preprocessing import LabelEncoder, StandardScaler

le = LabelEncoder()


for column in df.select_dtypes(include=['object']).columns:
  if column != 'Churn':
    df[column] = le.fit_transform(df[column])

#encode target variable

df['Churn'] = le.fit_transform(df['Churn'])

# scale numerical features

scaler = StandardScaler()

numerical_columns = ['tenure', 'MonthlyCharges', 'TotalCharges']

df[numerical_columns] = scaler.fit_transform(df[numerical_columns])

#featuress and target

X = df.drop('Churn', axis=1)
y = df['Churn']

from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
import lightgbm as lgb
from sklearn.metrics import accuracy_score, classification_report
from sklearn.metrics import roc_auc_score

# split dataset

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#apply smote

smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

#display
print("Class distribution after SMOTE: \n")
print(y_train_resampled.value_counts())

# train Random forest

rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train_resampled, y_train_resampled)
y_pred_rf = rf_model.predict(X_test)
roc_auc_rf = roc_auc_score(y_test, y_pred_rf)
print(f"Random Forest ROC-AUC: {roc_auc_rf}")

# train XGBoost

xgb_model = XGBClassifier(random_state=42)
xgb_model.fit(X_train_resampled, y_train_resampled)
y_pred_xgb = xgb_model.predict(X_test)
roc_auc_xgb = roc_auc_score(y_test, y_pred_xgb)
print(f"XGB ROC-AUC: {roc_auc_xgb}")

# Replace spaces and special characters in column names with underscores
import re

X_train_resampled = X_train_resampled.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '_', x))
X_test = X_test.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '_', x))

# Now, proceed with training the LightGBM model
lgb_model = lgb.LGBMClassifier(random_state=42)
lgb_model.fit(X_train_resampled, y_train_resampled)
y_pred_lgb = lgb_model.predict(X_test)
roc_auc_lgb = roc_auc_score(y_test, y_pred_lgb)
print(f"LightGB ROC-AUC: {roc_auc_lgb}")

# Train LightGBM

lgb_model = lgb.LGBMClassifier(random_state=42)
lgb_model.fit(X_train_resampled, y_train_resampled)
y_pred_lgb = lgb_model.predict(X_test)
roc_auc_lgb = roc_auc_score(y_test, y_pred_lgb)
print(f"LightGB ROC-AUC: {roc_auc_lgb}")

