# -*- coding: utf-8 -*-
"""Ensemble Learning101

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c_pgR7uHgHOU3BuLOpMfsztJg3zCdBQd
"""

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer
import torch

# 1️⃣ Load dataset for summarization
dataset = load_dataset("cnn_dailymail", "3.0.0")
print(dataset["train"][0])

# 2️⃣ Load tokenizer
tokenizer = AutoTokenizer.from_pretrained("t5-small")

# 3️⃣ Tokenize for summarization
def tokenize_function(examples):
    inputs = ["summarize: " + doc for doc in examples["article"]]
    model_inputs = tokenizer(inputs, max_length=512, truncation=True)

    # Tokenize targets (summaries)
    labels = tokenizer(text_target=examples["highlights"], max_length=150, truncation=True)

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Tokenize dataset in batches
tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=dataset["train"].column_names)

# 4️⃣ Load model
model = AutoModelForSeq2SeqLM.from_pretrained("t5-small")

# 5️⃣ Device setup (GPU or CPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")
model.to(device)

# 6️⃣ Training arguments
training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",  # For older versions, use eval_strategy; for new versions, use evaluation_strategy
    learning_rate=2e-5,
    per_device_train_batch_size=4,    # smaller batch for memory safety
    per_device_eval_batch_size=4,
    num_train_epochs=1,               # keep 1 for quick testing
    weight_decay=0.01,
    save_total_limit=2,
    logging_dir="./logs",
    logging_steps=10,
    report_to="none"                  # prevents wandb/tensorboard auto-logging
)

# 7️⃣ Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"].select(range(2000)),       # smaller subset for faster demo
    eval_dataset=tokenized_datasets["validation"].select(range(500)),
    processing_class=tokenizer  # ✅ new argument name replacing tokenizer
)

# 8️⃣ Train model
trainer.train()

# 9️⃣ Inference: Summarization
sample_text = "The Transformer model has revolutionized NLP by enabling parallel processing."
inputs = tokenizer(
    "summarize: " + sample_text,
    return_tensors="pt",
    max_length=512,
    truncation=True
)

# Move tensors to device
inputs = {k: v.to(device) for k, v in inputs.items()}

# Generate summary
outputs = model.generate(**inputs, max_length=150, num_beams=4, early_stopping=True)

print("Generated Summary:", tokenizer.decode(outputs[0], skip_special_tokens=True))