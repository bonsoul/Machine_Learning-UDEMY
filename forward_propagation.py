# -*- coding: utf-8 -*-
"""Forward Propagation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v1oO0356cYipATM1IM1yUBiuwk1QmUpI

# Understanding Forward Propagation

> Process by which input data flows through the layers of a NN to produce an output

> Input Layer : Accepts input features and passes them to the next layer.

> Hidden Layers : Compute weighted sums of inputs, apply biases and passs the result through activation function.

> Output Layer : Produces predictions, typically  using an activation function suitable for the task.
"""

import numpy as np

#define activation functions

def sigmoid(X):
  return 1 / (1 + np.exp(-X))

def tanh(X):
  return np.tanh(X)

def relu(X):
  return np.maximum(0, X)

def softmax(X):
  exp_x = np.exp(X)
  return exp_x / np.sum(axis=0, keepdims=True)

# forward pass function

def forward_pass(X, weights, biases, activation_functions):
  z = np.dot(weights, X) + biases
  a = activation_functions(z)
  return a

#example inputs

X = np.array([[0.5], [0.8]])
weights = np.array([[0.2, 0.4], [0.6,0.1]])
biases = np.array([[0.1], [0.2]])

# perfom forward pass

activations = {
    "Sigmoid": sigmoid,
    "Tanh": tanh,
    "Relu": relu,
    "Softmax": softmax
}

for name, func in activations.items():
  output = forward_pass(X, weights, biases, func)
  print(f"{name} Activation Output : \n{output}\n")

