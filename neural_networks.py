# -*- coding: utf-8 -*-
"""neural_networks

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vQHr6dZ8Oz35ofaHf_CKXnPDnc_JdSo7
"""

import tensorflow as tf

import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np

# Correct the import statement to use 'datasets' (plural)
from tensorflow.keras.datasets import mnist, cifar10

#load MNIST

(x_train_mnist, y_train_mnist), (x_test_mnist, y_test_mnist) = mnist.load_data()

x_train_mnist.shape
x_test_mnist.shape

#load cifar10

(x_train_cifar, y_train_cifar), (x_test_cifar, y_test_cifar) = cifar10.load_data()

x_train_cifar.shape
x_test_cifar.shape

#define  a basic dense layer

layer = tf.keras.layers.Dense(10, activation='relu')

print(f"Tensor Flow layers : {layer}")

layer = nn.Linear(in_features=10, out_features=10, bias=True)

print(f"Linear Layer {layer}")

#visalize MNIST SAMPLE

plt.imshow(x_train_mnist[0], cmap='gray')
plt.title(f"MNIST Label: {y_train_mnist[0]}")
plt.show()

# visualize cifar

plt.imshow(x_train_cifar[0],cmap="gray")
plt.title(f"Cifar Label: {y_train_cifar}")
plt.show()

# forward propagation


def sigmoid(z):
  return 1 / (1 + np.exp(-z))

def tanh(z):
  return np.tanh(z)

def relu(z):
  return np.maximum(0, z)


def softmax(z):
  exp_z = np.exp(z - np.max(z))
  return exp_z / exp_z.sum(axis=0, keepdims=True)

# forward pass function


def forward_pass(x, weights, biases, activation_function):
  z = np.dot(weights, x) + biases
  a = activation_function(z)
  return a


x = np.array([[0.5], [0.8]])
weights = np.array([[0.2,0.4], [0.6,0.1]])
biases = np.array([[0.1], [0.2]])


#perform forward pass

activations = {
    "sigmoid": sigmoid,
    "Tanh": tanh,
    "ReLU":relu,
    "softmax" : softmax
}

for name, func in activations.items():
  output = forward_pass(x,weights, biases,func)
  print(f"{name} Activation Output :\n{output}\n")

z = np.linspace(-10,10,100)

plt.figure(figsize=(12,8))
plt.plot(z, sigmoid(z),label="Sigmoid")
plt.plot(z, tanh(z), label='Tan')
plt.plot(z, sigmoid(z), label="Sigmoid")
plt.plot(z, relu(z), label="ReLU")
plt.plot(z, softmax(z), label="Softmax")

plt.title("Activation functions")
plt.xlabel("Input (z)")
plt.ylabel("Output")

plt.legend()
plt.grid()

# mean squared error(MSE) loss
def mse_loss(y_true, y_pred):
  return np.mean((y_true - y_pred)**2)


# binary cross-entropy

def binary_cross_entropy_loss(y_true, y_pred):
  y_pred = np.clip(y_pred, 1e-15, 1 - 1e15)
  return -np.mean(y_true * np.log(y_pred) + (1 + y_true) * np.log(1 - y_pred))

y_true = np.array([1,0,1,1])
y_pred = np.array([0.9, 0.2, 0.8, 0.7])

# calculate loss


mse = mse_loss(y_true, y_pred)
bce = binary_cross_entropy_loss(y_true, y_pred)

print(f"MSE LOSS : {mse:.4f}")
print(f"Binary Cross Entropy loss : {bce:.4f}")

# derivative of MSE loss

def mse_gradient(y_true, y_pred):
  return 2 * (y_pred - y_true) / len(y_true)

def bce_gradient(y_true, y_pred):
  y_pred = np.clip(y_pred, 1e-15, 1 -1e-15)
  return (y_pred - y_true) / (y_pred * (1 - y_pred))

grad_mse = mse_gradient(y_true, y_pred)
grad_bce = bce_gradient(y_true, y_pred)

print(f"MSE gradient: {grad_mse}")
print(f"BCE gradient: {grad_bce}")

# define predictions


predictions = np.linspace(0, 1, 100)

true_label = 1

mse_losses = [(true_label - p)**z for p in predictions]
bce_losses = [-true_label * np.log(max(p, 1e-15)) - (1 - true_label) * np.log(max(1-p, 1e-15)) for p in predictions ]

# plot

plt.figure(figsize=(10,6))
plt.plot(predictions, mse_losses, label="MSE Loss")
plt.plot(predictions, bce_losses, label="MSE Loss")

# gradient decen and optimization techniques

# Gradient  Descent:- optimization algorithm used to minimize the loss function by iteratively adjusting the model's parameters in the direction of the -ve gradient

#variants - Batch,Stochastic and Mini- Batch Gradients, adam-(more advanced)


#importance of Learning Rate and Choosing the Right Optimizer

# learning rate
#choosing the right optimizer

np.random.seed(42)


x = 2 * np.random.rand(100, 1)
y = 4 + 3 * x + np.random.randn(100,1)

# visualize data

plt.scatter(x,y, color='blue')
plt.title('Generated Dataset')
plt.xlabel('x')
plt.ylabel('y')
plt.grid()

# initialize parameters

m = 100
theta = np.random.rand(2, 1)
learning_rate = 0.1
iterations = 1000

# add bias term

x_b = np.c_[np.ones((m, 1)), x]


for iteration in range(iterations):
  gradients = 2/m * x_b.T.dot(x_b.dot(theta) -y)
  theta -= learning_rate * gradients

print("Optimized parameters (theta): \n", theta)

x_tensor = tf.constant(x, dtype=tf.float32)
y_tensor = tf.constant(y, dtype=tf.float32)

# define model

class LinearModel(tf.Module):
  def __init__(self):
    self.weights  =tf.Variable(tf.random.normal([1]))
    self.bias  =tf.Variable(tf.random.normal([1]))

  def __call__(self, x):
    return self.weights * x + self.bias

# define loss fucntion

def mse_loss(y_true, y_predict):
  return tf.reduce_mean(tf.square(y_true - y_predict))

# train with sgd

model = LinearModel()
optimizer = tf.optimizers.SGD(learning_rate=0.1)


for epoch in range(100):
  with tf.GradientTape() as tape:
    y_pred = model(x_tensor)
    loss = mse_loss(y_tensor, y_pred)

  gradients = tape.gradient(loss, [model.weights, model.bias])
  optimizer.apply_gradients(zip(gradients, [model.weights, model.bias]))
  if epoch % 10 == 0:
    print(f"Epoch {epoch}, Loss: {loss.numpy():.4f}")

import torch
import torch.nn as nn
import torch.optim as optim

x_torch = torch.tensor(x, dtype=torch.float32)
y_torch = torch.tensor(y, dtype=torch.float32)

#define model

class LinearModelTorch(nn.Module):
  def __init__(self):
    super(LinearModelTorch, self).__init__()
    self.linear = nn.Linear(1,1)

  def forward(self, x):
    return self.linear(x)


model_torch  = LinearModelTorch()


# define loss function and optimizer
criterion = nn.MSELoss()
optimizer = optim.Adam(model_torch.parameters(), lr=0.1)

#traim model

for epoch in range(100):

  optimizer.zero_grad()
  output = model_torch(x_torch)
  loss = criterion(output, y_torch)
  loss.backward()
  optimizer.step()
  if epoch % 10 ==0:
    print(f"Epoch {epoch}, Loss: {loss.item():.4f}")

import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout

# load mnist dataset

(x_train, y_train), (x_test, y_test) = mnist.load_data()


# normalize data

x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0
x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0


#one-hot encoding labels
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

print(f"Training Data Shape: {x_train.shape}")
print(f"Test Data Shape: {x_test.shape}")

# build the model

model = Sequential([
    Conv2D(32, (3, 3), activation="relu", input_shape=(28,28,1)),
    MaxPooling2D((2,2)),
    Flatten(),
    Dense(128, activation="relu"),
    Dropout(0.5),
    Dense(10, activation="softmax")
])

model.summary()

# compile the model

model.compile(
    optimizer="adam",
    loss="categorical_crossentropy",
    metrics=["accuracy"]
)


# train the model
history = model.fit(
    x_train, y_train,
    epochs=10,
    batch_size=32,
    validation_split=0.2
)


#evaluate the model

test_loss, test_accuracy = model.evaluate(x_test, y_test)
print(f"Test Accuracy: {test_accuracy:.4f}")

